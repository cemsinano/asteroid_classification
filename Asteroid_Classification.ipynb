{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asteroid Classification\n",
    "\n",
    "__Cem Sinan Ozturk__\n",
    "\n",
    "_October 6, 2020_\n",
    "\n",
    "\n",
    "In this project, I am going to perform a prediction analysis for classifying hazardous asteroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display #to display all columns\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_curve, plot_precision_recall_curve\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, average_precision_score\n",
    "from sklearn.metrics import f1_score, plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Models: \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "#warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA (Exploratory Data Analysis):\n",
    "\n",
    "Clean the data:\n",
    "- Drop perfectly correlated columns(expect one of them)\n",
    "- Columns with single values\n",
    "- Close Approach Date vs Epoch Date Close Approach : Same information in a different representation. Drop one of them.\n",
    "\n",
    "This data doesn't have `null` values but it is always better to make sure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/nasa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation between Features\n",
    "\n",
    "As we can see from the following visualization, there are a number of columns which are perfectly correlated, and for most of the cases, these columns are the different representation of the same information. Therefore, it is best to eleminate extra ones.\n",
    "\n",
    "For example, Est Diameter is given in km, m, miles and feet. We only need one of those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm').set_precision(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col_list = list(df.columns)\n",
    "extras = []\n",
    "for col1 in df.columns:\n",
    "    col_list.remove(col1)\n",
    "    for col2 in col_list:\n",
    "        if(df[col1].dtype in [\"int64\",\"float64\"] and df[col2].dtype in [\"int64\",\"float64\"]):\n",
    "                if(abs(df[col1].corr(df[col2])) > 0.99):\n",
    "                    print(col1,'-',col2, ':', df[col1].corr(df[col2]))\n",
    "                    extras.append(col2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_delete = set(extras)\n",
    "cols_to_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(cols_to_delete, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columns with single value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features with the same value for all the instances will not bring any value to our models. It is best to eleminate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_val_cols = [col for col in df.columns if df[col].nunique() == 1]\n",
    "single_val_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(single_val_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch Date Close Approach and Close Approach Date\n",
    "\n",
    "`Epoch Date Close Approach` is epoch/unix timestamp representation of `Close Approach Date`. Therefore, we should only have one of those columns. For now, I will remove `Close Approach Date` since `Epoch Date Close Approach` can directly be used in the models.\n",
    "\n",
    "Also, I may use the **year**, **month** and **day-of-year** values as features. These values may reveal any seasonal trend. (The same logic is relevant to other date columns) However, adding these features will require dummy variables (One-Hot-Encoding). It will greatly increase the number of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Epoch Date Close Approach'], axis=1, inplace=True) # or drop : Close Approach Date\n",
    "#df.drop(['Close Approach Date'], axis=1, inplace=True) # or drop : Epoch Date Close Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Close Approach Year'] = df['Close Approach Date'].astype('datetime64').dt.year.astype('str')\n",
    "df['Close Approach Month'] = df['Close Approach Date'].astype('datetime64').dt.month.astype('str')\n",
    "df['Close Approach Day of Year'] = df['Close Approach Date'].astype('datetime64').dt.dayofyear.astype('str')\n",
    "df.drop(['Close Approach Date'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Close Approach Date'].astype('datetime64').hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Close Approach Year'].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orbit Determination Date:\n",
    "\n",
    "Orbit Determination Date has values in the format of _\"%Y-%m-%d %H:%M%S\"_. Even if we are going to use this information for our model it seems using it in second precision is not what we may need. Since there are 2680 values in 4687 instance. Using in day precision may be a solution.\n",
    "\n",
    "However, I will omit this date column for now since it seems it is just the information of when the date is put onto the system. \n",
    "\n",
    "I may need to revisit here to explore this column, later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Orbit Determination Year Month'] = df['Orbit Determination Date'].astype('datetime64').dt.strftime('%Y-%m')\n",
    "df['Orbit Determination Year Month'].hist(xrot=40)\n",
    "plt.ylabel('Number of Instances')\n",
    "plt.xlabel('Date(Month-Year)')\n",
    "plt.title('Orbit Determination Year Month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Orbit Determination Date','Orbit Determination Year Month'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairplot for Remaining Features\n",
    "\n",
    "It is a crowded plot with too many features but it may be helpful for a quick look. We can see there are some candidates for outliers but I will keep it simple and skip this for now. \n",
    "\n",
    "Next plots are memory and time intensive so I will keep them as comments. (I ran it only once, normally I would not keep it in my analysis notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sns.pairplot(df, kind='reg', diag_kind='kde') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look on the large diameter (`Est Dia in KM(min)>3`) instances to see if there is anything to catch with eye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Est Dia in KM(min)']>3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, there one instance with `Est Dia in KM(min) >15` which is way out of the range of `Est Dia in KM(min)`. It has mean 0.2 and standard deviation of 0.35. Although there are other candidates it seems a very obvious outlier from the plots below. I will keep it for modelling but I will create a copy for visualization purposes in the below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dfcopy = df[df['Est Dia in KM(min)']<15]\n",
    "#sns.pairplot(dfcopy, kind='reg', diag_kind='kde') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the columns\n",
    "\n",
    "There are some preprocessing steps. In normal circumstances, missing values needs to be imputed. Here, no need for this step.\n",
    "\n",
    "Also, scaling or encoding would be other types of preprocessing steps that I will perform here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.loc[:, df.columns == 'Hazardous'].astype(int)\n",
    "X = df.loc[:, df.columns != 'Hazardous']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = X.select_dtypes(exclude=\"object\").columns\n",
    "categorical_features =  X.select_dtypes(include=\"object\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(steps=[\n",
    "                                      ('scaler', StandardScaler())\n",
    "                                    ])\n",
    "\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "                                          ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "                                         ])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "                                 transformers=[\n",
    "                                    ('num', numeric_transformer, numeric_features),\n",
    "                                    ('cat', categorical_transformer, categorical_features)\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection and Fitting:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hazardous_perc = np.mean(df['Hazardous'])*100\n",
    "print(\"%0.2f\" %hazardous_perc, \"% of the total astroids are hazardous in the data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, only 16.1% of the asteroids are hazardous. Therefore, the data is unbalanced, and we need to be careful when we are evaluating the models. For example, if a model predicts everything as non-hazardous and missed all the hazardous asteroids. The accuracy will be 83.9% but it is equal to doing nothing! Therefore, we need to keep this in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Data into Training, Test, and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainvalid, X_test, y_trainvalid, y_test = train_test_split(X, y, train_size=0.8, random_state=13)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_trainvalid, y_trainvalid, \n",
    "                                                          train_size=0.75, random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Measures\n",
    "\n",
    "In this example, we have unbalanced data, we need to be careful on accuracy. Since, number of correct predicted labels may not mean model is predicting useful results. Therefore, we have to be careful and get use of other measures from confusion matrix such as precision, recall or f1 score.  Since we need to be able to predict positive class (**Hazardous**) in this unbalanced data. Here the formulas for better remembering. \n",
    "\n",
    "$$Recall =  \\frac{TP}{TP+FN}$$\n",
    "\n",
    "$$Precision =  \\frac{TP}{TP+FP}$$\n",
    "\n",
    "$$F_1 =  \\frac{2* Precision * Recall}{Precision * Recall} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(model, \n",
    "                X_train, y_train,\n",
    "                X_valid, y_valid, \n",
    "                show = True\n",
    "               ):\n",
    "    \"\"\"\n",
    "    Returns train and validation error given a model\n",
    "    train and validation X and y portions\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: sklearn classifier model\n",
    "        The sklearn model\n",
    "    X_train: numpy.ndarray        \n",
    "        The X part of the train set\n",
    "    y_train: numpy.ndarray\n",
    "        The y part of the train set    \n",
    "    X_valid: numpy.ndarray        \n",
    "        The X part of the validation set\n",
    "    y_valid: numpy.ndarray\n",
    "        The y part of the validation set    \n",
    "    Returns\n",
    "    -------\n",
    "        train_err: float\n",
    "        test_err: float            \n",
    "    \"\"\" \n",
    "    #y_pred = model.predict(X_valid)\n",
    "    #auc = roc_auc_score(y_valid, y_pred)\n",
    "    train_err = (1-model.score(X_train, y_train))\n",
    "    valid_err = (1-model.score(X_valid, y_valid))\n",
    "    \n",
    "    if show: \n",
    "        print(\"Training error:   %.2f\" % train_err)\n",
    "        print(\"Validation error: %.2f\" % valid_err)\n",
    "     #   print(\"ROC-Area Under Curve: %.2f\" % auc )\n",
    "        print('\\n')\n",
    "    return train_err, valid_err  # ,auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_confusion_matrix_classification_report(model, X_valid, y_valid, \n",
    "                                                   labels=['Non hazardous', 'Hazardous']):\n",
    "    \"\"\"\n",
    "    Displays confusion matrix and classification report. \n",
    "    \n",
    "    Arguments\n",
    "    ---------     \n",
    "    model -- sklearn classifier model\n",
    "        The sklearn model\n",
    "    X_valid -- numpy.ndarray        \n",
    "        The X part of the validation set\n",
    "    y_valid -- numpy.ndarray\n",
    "        The y part of the validation set       \n",
    "\n",
    "    Keyword arguments:\n",
    "    -----------\n",
    "    labels -- list (default = ['Non fraud', 'Fraud'])\n",
    "        The labels shown in the confusion matrix\n",
    "    Returns\n",
    "    -------\n",
    "        None\n",
    "    \"\"\"\n",
    "    ### Display confusion matrix \n",
    "    disp = plot_confusion_matrix(model, X_valid, y_valid,\n",
    "                                 display_labels=labels,\n",
    "                                 cmap=plt.cm.Blues, \n",
    "                                 values_format = 'd')\n",
    "    disp.ax_.set_title('Confusion matrix for the dataset')\n",
    "\n",
    "    ### Print classification report\n",
    "    print(classification_report(y_valid, model.predict(X_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy Model for Baseline\n",
    "\n",
    "It will be useful to compare real classifiers. \n",
    "\n",
    "For more info, [sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create an empty dictionary to store all the results\n",
    "results_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(steps=[\n",
    "                                    # ('imputer', SimpleImputer(strategy='median')),\n",
    "                                      ('scaler', StandardScaler())\n",
    "                                    ])\n",
    "\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "                                          #('imputer', SimpleImputer(strategy='constant', \n",
    "                                          #                          fill_value='missing')),\n",
    "                                          ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "                                         ])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "                                 transformers=[\n",
    "                                    ('num', numeric_transformer, numeric_features),\n",
    "                                    ('cat', categorical_transformer, categorical_features)\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fitting baseline model: ')\n",
    "dummy = DummyClassifier()\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', dummy)])\n",
    "\n",
    "t = time.time()\n",
    "clf.fit(X_train, y_train)\n",
    "t_fit = time.time() \n",
    "tr_err, valid_err = get_scores(clf, X_train, y_train, X_valid, y_valid)\n",
    "cross_val = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "y_pred = clf.predict(X_valid)\n",
    "#auc= roc_auc_score(y_valid, y_pred)\n",
    "f1_sc = f1_score(y_valid, y_pred)\n",
    "t_predict = time.time() \n",
    "\n",
    "time_to_fit = t_fit - t\n",
    "time_to_predict = t_predict - t_fit\n",
    "    \n",
    "\n",
    "results_dict['dummy'] = [round(tr_err,3), \"%0.2f (+/- %0.2f)\" %(cross_val.mean(), cross_val.std() * 2), round(valid_err,3), round(f1_sc,3), round(time_to_fit,4), round(time_to_predict,4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers to Evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "          'decision tree': DecisionTreeClassifier(),\n",
    "          'kNN': KNeighborsClassifier(),\n",
    "          'logistic regression': LogisticRegression(),\n",
    "          'RBF SVM' : SVC(), \n",
    "          'random forest' : RandomForestClassifier(), \n",
    "          'xgboost' : XGBClassifier(),\n",
    "          'lgbm': LGBMClassifier()\n",
    "         }\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    t = time.time()\n",
    "    #print(model_name, \":\")    \n",
    "    clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                          ('classifier', model)])\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    tr_err, valid_err = get_scores(clf, X_train, y_train, \n",
    "                                   X_valid, y_valid, show = False)\n",
    "    cross_val = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "    t_fit = time.time() \n",
    "    y_pred = clf.predict(X_valid)\n",
    "    #auc= roc_auc_score(y_valid, y_pred)\n",
    "    f1_sc = f1_score(y_valid, y_pred)\n",
    "    t_predict = time.time() \n",
    "    \n",
    "    time_to_fit = t_fit - t\n",
    "    time_to_predict = t_predict - t_fit\n",
    "    \n",
    "    results_dict[model_name] = [round(tr_err,3),\"%0.2f (+/- %0.2f)\" % (cross_val.mean(), cross_val.std() * 2), round(valid_err,3), round(f1_sc,3), round(time_to_fit,4), round(time_to_predict,4)]\n",
    "    #print(\"Elapsed time: %.1f s\" % elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results_dict).T\n",
    "results_df.columns = [\"Train error\",\"Training CV Accuracy\",  \"Validation error\", \"F1 Score on Validation\", \"Fit Time in sec\", \"Predict Time in sec\"]\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SVM and kNN are slow models. So, it is best to eleminate them. Random Forest is also slow to fit and it is overfitted. \n",
    "- Decision Tree, Random Forest and boosted models(xgboost and lgbm) seems overfitted. Training errors are 0. We can apply hyperparameter optimization.\n",
    "\n",
    "I prefer to continue with Logistic Regression. It is the fastest model with reasonable accuracy and AUC values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization on Selected Model (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameter optimization \n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression())])\n",
    "\n",
    "### We can also choose optimal class_weight ratio using grid search. \n",
    "weights = np.linspace(0.05, 0.95, 20)\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__C': [0.01, 0.1, 1.0, 10, 100],\n",
    "    #'classifier__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'classifier__class_weight': [{0: x, 1: 1.0-x} for x in weights]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(clf,\n",
    "                           param_grid=param_grid,\n",
    "                           scoring='f1',\n",
    "                           cv=10)\n",
    "grid_search.fit(X_trainvalid, y_trainvalid)\n",
    "\n",
    "print((\"best logistic regression from grid search (F1 Score): %.3f\"\n",
    "       % grid_search.score(X_valid, y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters : %s\" % grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion_matrix_classification_report(grid_search, X_valid, y_valid, \n",
    "                                                   labels=['Non hazardous', 'Hazardous'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation scores of 0.89 precision and 0.88 recall on hazardous class seems reasonably well. I will continue to use this Logistic Regression model for predicting unseen test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Results on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion_matrix_classification_report(grid_search, X_test, y_test, \n",
    "                                                   labels=['Non hazardous', 'Hazardous'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both precision and recall for the hazardous class have fallen to 0.83 and 0.82, respectively. Also, the accuracy score has fallen from 96% to 94%. However, the model results still seem reasonable. Based on our test set, we will be able to identify 132 (~83%) hazardous asteroids out of a total of 159 (132+27) hazardous asteroids. I will come with the cost of 29 *non-hazardous* to be misidentified as *hazardous*. \n",
    "\n",
    "If we want to increase the precision of classifying hazardous asteroids, it will cost a reduction in recall which means we will have more *non-hazardous* to be misidentified as *hazardous*. In the case of NASA, if we have more asteroids predicted as hazardous, it may increase the cost of tracking those asteroids and other studies. However, it will give peace of mind since a missed hazardous asteroid may result in massive natural destruction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements and/or Other Approaches\n",
    "\n",
    "Since we have unbalanced data, we could also perform:\n",
    "\n",
    "- Undersampling: It is mainly undersampling the \"non-hazardous\" class to get balanced data and fit model on this.\n",
    "- Oversampling: It is oversampling the minority class(\"hazardous\"). There are different techniques for oversampling. One is random oversampling weith replacement. Another model is SMOTE (Synthetic Minority Over-sampling Technique). SMOTE creates \"synthetic\" instances instead of oversampling with replacement.\n",
    "\n",
    "Here, I have selected Logistic Regression since it is fast and interpretable model. Boosted models with hyperparameter optimization will tends to give better results. This type of models can be another way to go. \n",
    "\n",
    "Last but not least, making the analysis reproducible by automating some processes would be beneficial for future similar works. For the sake of simplicity, I keep all the code and analysis in a single JupyterNotebook. For the best practices, my next step would be breakdown down the code into scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
